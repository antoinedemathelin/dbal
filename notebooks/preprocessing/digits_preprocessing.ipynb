{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Conv2D, MaxPooling2D, Flatten, Reshape, GaussianNoise\n",
    "from tensorflow.keras.constraints import MinMaxNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First downloas mnist and svhn datasets:\n",
    "- mnist: http://yann.lecun.com/exdb/mnist/\n",
    "- svhn: http://ufldl.stanford.edu/housenumbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../datasets/digits/svhn_X.npy\")\n",
    "y = np.load(\"../datasets/digits/svhn_y.npy\")\n",
    "\n",
    "Xt = np.load(\"../datasets/digits/mnist_X.npy\")\n",
    "yt = np.load(\"../datasets/digits/mnist_y.npy\")\n",
    "\n",
    "# Normalize the data to get values between (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    modeled = Conv2D(32, 5, activation='relu')(inputs)\n",
    "    modeled = MaxPooling2D(2, 2)(modeled)\n",
    "    modeled = Conv2D(48, 5, activation='relu')(modeled)\n",
    "    modeled = MaxPooling2D(2, 2)(modeled)\n",
    "    modeled = Flatten()(modeled)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=\"adam\", loss='mse')\n",
    "    return model\n",
    "\n",
    "def get_task(input_shape, output_shape=(10,), activation=\"softmax\", C=1.):\n",
    "    inputs = Input(input_shape)\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    modeled = Dense(np.prod(output_shape), activation=activation)(modeled)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=\"adam\", loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 768)               39280     \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 10)                88010     \n",
      "=================================================================\n",
      "Total params: 127,290\n",
      "Trainable params: 127,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((28, 28, 1))\n",
    "encoder = get_encoder((28, 28, 1))\n",
    "task = get_task(encoder.output_shape[1:])\n",
    "\n",
    "encoded = encoder(inputs)\n",
    "tasked = task(encoded)\n",
    "\n",
    "model = Model(inputs, tasked)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"svhn\"\n",
    "\n",
    "X = X.reshape(X.shape + (1,))\n",
    "one = OneHotEncoder(sparse=False)\n",
    "y_cat = one.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 32s 539us/sample - loss: 0.6494 - accuracy: 0.7950\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 33s 543us/sample - loss: 0.2502 - accuracy: 0.9250\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 32s 528us/sample - loss: 0.1808 - accuracy: 0.9449\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 33s 542us/sample - loss: 0.1409 - accuracy: 0.9566\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 32s 537us/sample - loss: 0.1168 - accuracy: 0.9636\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 31s 524us/sample - loss: 0.0975 - accuracy: 0.9696\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 32s 530us/sample - loss: 0.0889 - accuracy: 0.9720\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 0.0778 - accuracy: 0.9752\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 31s 521us/sample - loss: 0.0728 - accuracy: 0.9768\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 0.0653 - accuracy: 0.9798\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 31s 513us/sample - loss: 0.0583 - accuracy: 0.9811\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 31s 516us/sample - loss: 0.0553 - accuracy: 0.9820\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 0.0480 - accuracy: 0.9848\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 31s 520us/sample - loss: 0.0475 - accuracy: 0.9846\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 0.0491 - accuracy: 0.9844\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 0.0436 - accuracy: 0.9860\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 31s 510us/sample - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 31s 513us/sample - loss: 0.0389 - accuracy: 0.9877\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 0.0366 - accuracy: 0.9883\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 0.0378 - accuracy: 0.9878\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 31s 515us/sample - loss: 0.0331 - accuracy: 0.9894\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 31s 519us/sample - loss: 0.0331 - accuracy: 0.9893\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 31s 521us/sample - loss: 0.0321 - accuracy: 0.9899\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 31s 515us/sample - loss: 0.0337 - accuracy: 0.9891\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 31s 515us/sample - loss: 0.0317 - accuracy: 0.9901\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 0.0275 - accuracy: 0.9912\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - 31s 520us/sample - loss: 0.0320 - accuracy: 0.9898\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 31s 515us/sample - loss: 0.0300 - accuracy: 0.9908\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 31s 515us/sample - loss: 0.0267 - accuracy: 0.9911\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 31s 518us/sample - loss: 0.0240 - accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x267804fce88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "model.fit(X, y_cat, batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"datasets/digits_preprocessed/svhn_X.npy\", encoder.predict(X))\n",
    "np.save(\"datasets/digits_preprocessed/minst_X.npy\", encoder.predict(Xt))\n",
    "\n",
    "np.save(\"datasets/digits_preprocessed/svhn_y.npy\", y)\n",
    "np.save(\"datasets/digits_preprocessed/minst_y.npy\", yt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (statmath)",
   "language": "python",
   "name": "statmath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
